{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sefeoglu/neurogenesis-cre/blob/master/Neurogenesis_Memory_Replay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0c-IPhqs6f-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade pip\n",
        "!pip3 install --upgrade transformers\n",
        "!pip3 install --upgrade accelerate\n",
        "!pip3 install sentencepiece\n",
        "!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr --upgrade\n",
        "!pip install ipywidgets\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "!pip install evaluate\n",
        "!pip install trl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install transformers==4.45.2"
      ],
      "metadata": {
        "id": "Dye4g2vFZmop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda36f34-d7b2-432e-9ec7-bf6bea5937cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed transformers-4.45.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip relations.zip\n",
        "!unzip task_data.zip"
      ],
      "metadata": {
        "id": "ncY-1M2DXVPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxzzoufELW2j",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !unzip relations.zip\n",
        "!unzip memory_based_same_cre.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf memory_based_same_cre_related_work"
      ],
      "metadata": {
        "id": "l_weBDai-WUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6wTxudv0_zR"
      },
      "outputs": [],
      "source": [
        "!zip memory_based_cre.zip -r memory_based_cre/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTuKyhYz0_zR",
        "outputId": "f7c9a016-7522-4d68-a6ef-27e0c4015338"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'transformers.optimization' from '/opt/conda/lib/python3.11/site-packages/transformers/optimization.py'>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF3bt_RcT7Hg",
        "outputId": "ce852dc0-cc0e-4001-d7f2-64de25ec74fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Continuous instruction fine-tuning of Flan T5 model for Relation Extraction\"\"\"\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from random import randrange\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments\n",
        "from peft import  get_peft_model, LoraConfig, TaskType\n",
        "from transformers import BitsAndBytesConfig\n",
        "from huggingface_hub import HfFolder\n",
        "import evaluate\n",
        "import nltk, torch\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Metric\n",
        "metric = evaluate.load(\"rouge\")\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def set_dataset(dataset_id):\n",
        "    # Load dataset\n",
        "\n",
        "    dataset = load_dataset(dataset_id)\n",
        "\n",
        "    print(len(dataset['validation']))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def set_tokenizer(model_id=\"google/flan-t5-base\"):\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "tokenizer = set_tokenizer()\n",
        "def preprocess_function(sample,padding=\"max_length\"):\n",
        "    # add prefix to the input for t5\n",
        "    inputs = [item for item in sample[\"prompt\"]]\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `text_target` keyword argument\n",
        "    labels = tokenizer(text_target=sample[\"relation\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# huggingface hub model id\n",
        "def load_model(model_id=\"google/flan-t5-base\", local=False):\n",
        "\n",
        "    compute_dtype = getattr(torch, \"float16\")\n",
        "    # load model from the hub\n",
        "    # maxmem={i:f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB' for i in range()}\n",
        "    # maxmem['cpu']='300GB'\n",
        "    bnb_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_use_double_quant=False,\n",
        "    )\n",
        "\n",
        "    # model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  device_map=\"auto\", max_memory=maxmem, quantization_config=bnb_config)\n",
        "    if local:\n",
        "      model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  device_map=\"auto\", quantization_config=bnb_config, local_files_only=True)\n",
        "    else:\n",
        "      model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  device_map=\"auto\", quantization_config=bnb_config)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "  if isinstance(logits, tuple):\n",
        "    logits = logits[0]\n",
        "\n",
        "  return logits.argmax(dim=-1)\n",
        "# helper function to postprocess text\n",
        "def postprocess_text(labels, preds):\n",
        "    preds = [pred.replace('\\n','').split('Answer:')[-1].strip() for pred in preds]\n",
        "    labels = [label.replace('\\n','').split('Answer:')[-1].strip() for label in labels]\n",
        "    #print(preds)\n",
        "    #print(labels)\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    targets_labels = read_json(tasks_path)\n",
        "    # Replace -100 in the preds as we can't decode them\n",
        "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "\n",
        "    # Decode generated summaries into text\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    # Decode reference summaries into text\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # ROUGE expects a newline after each sentence\n",
        "        # Some simple post-processing\n",
        "\n",
        "    # model_predictions.extend(decoded_preds)\n",
        "    grounds, preds = postprocess_text(decoded_labels,decoded_preds)\n",
        "    p, r, f, _ = precision_recall_fscore_support(grounds, preds, labels=targets_labels, average='micro')\n",
        "    acc = accuracy_score(grounds, preds)\n",
        "    decoded_preds = [\"\\n\".join(pred.strip()) for pred in decoded_preds]\n",
        "\n",
        "    decoded_labels = [\"\\n\".join(label.strip()) for label in decoded_labels]\n",
        "    # Compute ROUGscores\n",
        "    result = metric.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "    # Extract the median scores\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    result['f1'] = f\n",
        "    result['recall'] =r\n",
        "    result['precision']=p\n",
        "    result['acc']=acc\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "def main(model_id, data_path, tasks_path, task_id, local):\n",
        "\n",
        "    targets_labels = read_json(tasks_path)\n",
        "    print(targets_labels)\n",
        "\n",
        "\n",
        "\n",
        "    dataset = set_dataset(data_path)\n",
        "    model = load_model(model_id, local)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    tokenizer = set_tokenizer(model_id=\"google/flan-t5-base\")\n",
        "\n",
        "    # The maximum total input sequence length after tokenization.\n",
        "    # Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "    tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(lambda x: tokenizer(x[\"prompt\"], truncation=True), batched=True, remove_columns=[\"prompt\", \"relation\"])\n",
        "    max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
        "    print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "    # The maximum total sequence length for target text after tokenization.\n",
        "    # Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "    tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(lambda x: tokenizer(x[\"relation\"], truncation=True), batched=True, remove_columns=[\"prompt\", \"relation\"])\n",
        "    max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
        "    print(f\"Max target length: {max_target_length}\")\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"prompt\", \"relation\"])\n",
        "    print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
        "\n",
        "    # model = load_model(model_id, local)\n",
        "    # model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "\n",
        "    # we want to ignore tokenizer pad token in the loss\n",
        "    label_pad_token_id = -100\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8\n",
        "    )\n",
        "\n",
        "\n",
        "    # Hugging Face repository id\n",
        "    repository_id = f\"{model_id}-{task_id}\"\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=repository_id,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        predict_with_generate=True,\n",
        "        fp16=False, # Overflows with fp16\n",
        "        learning_rate=1e-3,\n",
        "        num_train_epochs=5,\n",
        "        # logging & evaluation strategies\n",
        "        logging_dir=f\"{repository_id}/logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=500,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        # push to hub parameters\n",
        "        report_to=\"tensorboard\",\n",
        "        push_to_hub=False,\n",
        "        hub_strategy=\"every_save\",\n",
        "        hub_model_id=repository_id,\n",
        "        hub_token=HfFolder.get_token(),\n",
        "        lr_scheduler_type = \"cosine_with_restarts\",\n",
        "        lr_scheduler_kwargs = { \"num_cycles\": 1 },\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "    # Create Trainer instance\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"validation\"],\n",
        "        compute_metrics=compute_metrics,\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    trainer.model.save_pretrained(repository_id)\n",
        "    merged_model = model.merge_and_unload()\n",
        "\n",
        "    return merged_model, tokenizer, trainer\n",
        "def read_json(path):\n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(data, path):\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "def get_prediction(model,tokenizer, prompt, length=250,stype='greedy'):\n",
        "\n",
        "    inputs = tokenizer(prompt, add_special_tokens=True, max_length=4096,return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(inputs, max_new_tokens=length)\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "fCKua59IFgDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHFZlufnBlGQ"
      },
      "outputs": [],
      "source": [
        "def read_json(path):\n",
        "    \"\"\" Read a json file from the given path.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(data, path):\n",
        "    \"\"\" Write a json file to the given path.\"\"\"\n",
        "    if not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QETADxqEvEk"
      },
      "outputs": [],
      "source": [
        "##kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "#send data emmbeddings according to relation types.\n",
        "def sample_selection_kmeans(data, memory_size):\n",
        "    # Extract the embeddings and convert them into a 2D numpy array\n",
        "    embeddings_array = [item['embedding'] for item in data]\n",
        "    prompt_array = [item['prompt'] for item in data]\n",
        "    relation_array = [item['relation'] for item in data]\n",
        "\n",
        "    min_dim = min(emb.shape[1] for emb in embeddings_array)\n",
        "\n",
        "    reshaped_embeddings = [emb[:, :min_dim].reshape(-1) for emb in embeddings_array]\n",
        "    embeddings_array = np.vstack(reshaped_embeddings)\n",
        "\n",
        "    num_clusters = min(memory_size, len(embeddings_array))\n",
        "\n",
        "    distances = KMeans(n_clusters=num_clusters, random_state=0).fit_transform(embeddings_array)\n",
        "\n",
        "    mem_set = [] # Initialize mem_set as a list\n",
        "    proto_set = []\n",
        "    relations = []\n",
        "    selected_samples = []\n",
        "\n",
        "    for i in range(num_clusters):\n",
        "        sel_index = np.argmin(distances[:,i])\n",
        "        instance = embeddings_array[sel_index]\n",
        "        mem_set.append(instance)\n",
        "        proto = prompt_array[sel_index]\n",
        "        proto_set.append(proto)\n",
        "        relations.append(relation_array[sel_index])\n",
        "        print(relation_array[sel_index])\n",
        "        selected_samples.append({\"prompt\":proto,\"relation\":relation_array[sel_index]})\n",
        "\n",
        "\n",
        "    mem_set = np.array(mem_set)\n",
        "\n",
        "    return mem_set, proto_set, relations, selected_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJAmobReEyYS"
      },
      "outputs": [],
      "source": [
        "##embedding computation\n",
        "def compute_embedding(model, tokenizer, input_path):\n",
        "\n",
        "  data = read_json(input_path)\n",
        "\n",
        "  embeddings = []\n",
        "  for i, item in enumerate(data):\n",
        "    prompt = item['prompt']\n",
        "    relation  = item['relation']\n",
        "    inputs = tokenizer(prompt, add_special_tokens=True, max_length=4096,return_tensors=\"pt\").input_ids\n",
        "    embedding = model.encoder(inputs)\n",
        "\n",
        "    embeddings.append({\"prompt\": prompt, \"relation\":relation, \"embedding\": embedding['last_hidden_state'].data.numpy()})\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-3ig6g3QcmN"
      },
      "outputs": [],
      "source": [
        "def select_samples(model, tokenizer, memory_size, train_data_path, tasks_path):\n",
        "  ### TODO ###\n",
        "  relations = read_json(tasks_path)\n",
        "\n",
        "  embeddings = compute_embedding(model, tokenizer, train_data_path)\n",
        "  r1_emb = [ emb for emb in embeddings if emb['relation'] == relations[-1]]\n",
        "  r2_emb = [ emb for emb in embeddings if emb['relation'] == relations[-2]]\n",
        "  r3_emb = [ emb for emb in embeddings if emb['relation'] == relations[-3]]\n",
        "  r4_emb = [ emb for emb in embeddings if emb['relation'] == relations[-4]]\n",
        "  mem1, proto_set1, relations1,selected_samples1 = sample_selection_kmeans(r1_emb, 20)\n",
        "  mem2, proto_set2, relations2, selected_samples2 = sample_selection_kmeans(r2_emb, 20)\n",
        "  mem3, proto_set3, relations3, selected_samples3 = sample_selection_kmeans(r3_emb, 20)\n",
        "  mem4, proto_set4, relations4, selected_samples4 = sample_selection_kmeans(r4_emb, 20)\n",
        "  all_selected_samples = []\n",
        "  all_selected_samples.extend(selected_samples1)\n",
        "  all_selected_samples.extend(selected_samples2)\n",
        "  all_selected_samples.extend(selected_samples3)\n",
        "  all_selected_samples.extend(selected_samples4)\n",
        "  all_mem, saved_mem = [],[]\n",
        "  all_mem.extend(mem1)\n",
        "  all_mem.extend(mem2)\n",
        "  all_mem.extend(mem3)\n",
        "  all_mem.extend(mem4)\n",
        "\n",
        "  for i, mem in enumerate(all_mem):\n",
        "     # Change here: Access the 'prompt' value from the dictionary\n",
        "     # to get the actual prompt string\n",
        "     saved_mem.append({\"prompt\":all_selected_samples[i]['prompt'], \"embedding\":mem})\n",
        "\n",
        "  if os.path.exists('selected_samples.npy'):\n",
        "    old_mem = np.load('selected_samples.npy', allow_pickle=True)\n",
        "    saved_mem.extend(old_mem)\n",
        "  np.save('selected_samples.npy', saved_mem)\n",
        "  return all_selected_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_encoder(data, model, tokenizer):\n",
        "    optimizer = torch.optim.AdamW(model.encoder.parameters(), lr=1e-3)\n",
        "\n",
        "    # Assuming 'batch['prompt']' contains the text for which you want to get embeddings\n",
        "    for epoch in range(10):\n",
        "        print(\"Epoch: {0}\".format(epoch))\n",
        "        model.encoder.train()\n",
        "        for batch in data:\n",
        "\n",
        "          with torch.enable_grad():\n",
        "            # Tokenize the prompt to get input IDs\n",
        "            inputs = tokenizer(batch['prompt'], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model.get_encoder()(inputs)\n",
        "\n",
        "            # Convert NumPy array to PyTorch tensor and requires_grad to True\n",
        "            # Convert target embeddings to float16 to match model's dtype\n",
        "            target_embedding = torch.tensor(batch['embedding'], dtype=torch.float16, requires_grad=True).to(\"cuda\")\n",
        "\n",
        "            # Check the shape of target_embedding and outputs.last_hidden_state\n",
        "            # print(\"Shape of target_embedding:\", target_embedding.shape)\n",
        "            # print(\"Shape of outputs.last_hidden_state:\", outputs.last_hidden_state.shape)\n",
        "\n",
        "            # Flatten target_embedding and select the corresponding elements from outputs.last_hidden_state\n",
        "            target_embedding = target_embedding.view(-1)  # Flatten to 1D\n",
        "\n",
        "            # Get the total number of elements in target_embedding\n",
        "            num_elements = target_embedding.shape[0]\n",
        "\n",
        "            # Select the first 'num_elements' elements from the flattened outputs.last_hidden_state\n",
        "            output_slice = outputs.last_hidden_state.view(-1)[:num_elements]\n",
        "\n",
        "            # Reshape output_slice to match the original shape of target_embedding if necessary\n",
        "            output_slice = output_slice.view(target_embedding.shape)\n",
        "\n",
        "            # Calculate loss using MSE\n",
        "            loss = torch.nn.MSELoss()(output_slice, target_embedding)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "            print(\"batch loss: {0}\".format(loss))"
      ],
      "metadata": {
        "id": "Mx8tNXPGnM4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(experiment_id, task_id, model, tokenizer, current_task=True):\n",
        "    if current_task:\n",
        "      input_path = \"memory_based_cre/test/run_{0}/task{1}/test_1.json\".format(experiment_id, task_id)\n",
        "      data = read_json(input_path)\n",
        "      out_pred_path = \"KMmeans_CRE_{0}/task_{1}_current_task_pred.json\".format(experiment_id, task_id)\n",
        "      out_acc_path = \"KMmeans_CRE_{0}/task_{1}_current_task_result.json\".format(experiment_id, task_id)\n",
        "    else:\n",
        "      data = []\n",
        "      for i in range(1, task_id+1):\n",
        "        input_path = \"memory_based_cre/test/run_{0}/task{1}/test_1.json\".format(experiment_id, i)\n",
        "        data.extend(read_json(input_path))\n",
        "      out_pred_path = \"KMmeans_CRE_{0}/task_{1}_seen_task.json\".format(experiment_id, task_id)\n",
        "      out_acc_path = \"KMmeans_CRE_{0}/task_{1}_seen_task_result.json\".format(experiment_id, task_id)\n",
        "\n",
        "    responses = []\n",
        "    relations = []\n",
        "    for j, item in enumerate(data):\n",
        "      prompt = item['prompt']\n",
        "      relations.append(item['relation'])\n",
        "\n",
        "      response = get_prediction(model, tokenizer, prompt)\n",
        "      print('test:', j)\n",
        "    if len(response) == 0:\n",
        "      print(\"No response\")\n",
        "      responses.append(\"\")\n",
        "    else:\n",
        "      print(response[0])\n",
        "      responses.append({\"predict\":response[0]})\n",
        "  y_true = relations\n",
        "  preds = [line['predict'] for line in responses]\n",
        "  acc = accuracy_score(y_true, preds)\n",
        "  result = [{\"acc\":acc}]\n",
        "\n",
        "  write_json(responses, out_pred_path)\n",
        "  write_json(result,out_acc_path)"
      ],
      "metadata": {
        "id": "L47j0Z3shXLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCkAsF_2Ux_5",
        "scrolled": true,
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "for experiment_id in range(1, 6):\n",
        "\n",
        "  print(\"Experiment: {0}\".format(experiment_id))\n",
        "\n",
        "  dataset_path = 'memory_based_cre/train/run_{0}/task1/'.format(experiment_id)\n",
        "  tasks_path = \"relations/run_{0}/task1.json\".format(experiment_id)\n",
        "  model_id=\"google/flan-t5-base\"\n",
        "  task_id = \"task1\"\n",
        "  targets_labels = []\n",
        "  max_source_length = None\n",
        "  max_target_length = None\n",
        "  tokenizer = set_tokenizer()\n",
        "\n",
        "  lora_config = LoraConfig(\n",
        "                  # the task to train for (sequence-to-sequence language modeling in this case)\n",
        "                  task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "                  # the dimension of the low-rank matrices\n",
        "                  r=4,\n",
        "                  # the scaling factor for the low-rank matrices\n",
        "                  lora_alpha=32,\n",
        "                  # the dropout probability of the LoRA layers\n",
        "                  lora_dropout=0.01,\n",
        "                  target_modules=[\"k\",\"q\",\"v\",\"o\"]\n",
        "                  )\n",
        "  metric = evaluate.load(\"rouge\")\n",
        "  model, tokenizer, trainer = main(model_id, dataset_path, tasks_path, task_id, False)\n",
        "\n",
        "  model.save_pretrained(\"KMmeans_CRE_{0}/task_memory_20_1/\".format(experiment_id), from_pt=True)\n",
        "  ## evaluate model\n",
        "  evaluate_model(experiment_id, task_id, model, tokenizer, current_task=True)\n",
        "\n",
        "  train_data_path = dataset_path+\"train_1.json\"\n",
        "  all_selected_samples = select_samples(model, tokenizer, 20, train_data_path, tasks_path)\n",
        "\n",
        "  for k in range(2, 11):\n",
        "    outpath_selected_samples = 'memory_based_cre/train/run_{0}/task_memory_{1}/train_2.json'.format(experiment_id,k)\n",
        "    write_json(all_selected_samples, outpath_selected_samples)\n",
        "\n",
        "  for i in range(1, 10):\n",
        "    targets_labels = []\n",
        "    max_source_length = None\n",
        "    max_target_length = None\n",
        "    tokenizer = set_tokenizer()\n",
        "    metric = evaluate.load(\"rouge\")\n",
        "\n",
        "    dataset_path = 'memory_based_cre/train/run_{0}/task{1}/'.format(experiment_id,i+1)\n",
        "    tasks_path = \"relations/run_{0}/task{1}.json\".format(experiment_id,i+1)\n",
        "\n",
        "    base_model_id=\"KMmeans_CRE_{0}/task_memory_20_{1}/\".format(experiment_id, i)\n",
        "    task_id = \"task{0}\".format(i+1)\n",
        "\n",
        "    print(base_model_id)\n",
        "\n",
        "    model, tokenizer, trainer = main(base_model_id, dataset_path, tasks_path, task_id, True)\n",
        "    #evaluate model\n",
        "    evaluate_model(experiment_id, task_id, model, tokenizer, current_task=True)\n",
        "\n",
        "    if i <9:\n",
        "      train_data_path = dataset_path+\"train_1.json\"\n",
        "      all_selected_samples = select_samples(model, tokenizer,20, train_data_path, tasks_path)\n",
        "      for j in range(i+2, 11):\n",
        "        outpath_selected_samples = 'memory_based_cre/train/run_{0}/task_memory_{1}/train_{2}.json'.format(experiment_id, j, i+2)\n",
        "        write_json(all_selected_samples, outpath_selected_samples)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "    ########################### Memory Train ######################################\n",
        "    targets_labels = []\n",
        "    max_source_length = None\n",
        "    max_target_length = None\n",
        "    tokenizer = set_tokenizer()\n",
        "    metric = evaluate.load(\"rouge\")\n",
        "\n",
        "    dataset_path = 'memory_based_cre/train/run_{0}/task_memory_{1}/'.format(experiment_id,i+1)\n",
        "    tasks_path = \"relations/run_{0}/task{1}.json\".format(experiment_id,i+1)\n",
        "\n",
        "    base_model_id = \"KMmeans_CRE_{0}/task_memory_20_{1}/\".format(experiment_id, i+1)\n",
        "    task_id = \"task{0}\".format(i+1)\n",
        "\n",
        "    print(base_model_id)\n",
        "\n",
        "    model, tokenizer, trainer = main(base_model_id, dataset_path, tasks_path, task_id, True)\n",
        "    model.save_pretrained(\"KMmeans_CRE_{0}/task_memory_20_{1}/\".format(experiment_id, i+1), from_pt=True)\n",
        "    ### evaluate model\n",
        "    evaluate_model(experiment_id, task_id, model, tokenizer, current_task=False)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "beAdIPwHlJeK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BTFvW2kPV2dC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}