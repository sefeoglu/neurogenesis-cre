{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1BZy6O0sWL7"
      },
      "outputs": [],
      "source": [
        "!unzip tacred.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D9rQON1KJy0"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/trl\n",
        "!pip install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R7_BW2EKNUk",
        "outputId": "6da55954-810c-4d26-a438-cef2e85ca07e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `gmm` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `gmm`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token 'your_token_here'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGykAavP9S83"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lIRlBhsXjtJT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import BertModel, PreTrainedModel, BertTokenizer, BertConfig\n",
        "from transformers import BertForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2_7r_absZ-G"
      },
      "source": [
        "## Source Codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jCdQi2LssGj6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_phi(m, D, which_phi='performer', device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"\n",
        "    Function that returns the random feature map, phi.\n",
        "    Since our neuron-astrocyte model is equivalent to using Random Feature Attention,\n",
        "    we use this representation for simplicity. Different phi functions lead to different feature maps.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Random weight matrix for random feature map\n",
        "    W = torch.randn((m, D), device=device) # Changed W initialization to (m, D)\n",
        "\n",
        "    if which_phi == 'cosine':\n",
        "        # Random biases for cosine feature map\n",
        "        rand_b = torch.rand(m, device=device) * 2 * torch.pi\n",
        "\n",
        "        def phi(x, c=0):\n",
        "            \"\"\"Uses a cosine random feature map to approximate softmax attention.\"\"\"\n",
        "\n",
        "            projected_x = nn.Linear(x.shape[-1], D, bias=False).to(device)(x) # Project x to dimension D using a linear layer with no bias\n",
        "            # projected_x.shape is now (sequence_length, D) which is (512, 512)\n",
        "\n",
        "            return torch.sqrt(torch.tensor(2 / m, device=device)) * torch.cos(projected_x @ W + rand_b) * torch.exp(0.5 * (torch.norm(projected_x) ** 2) - c) # Changed W @ projected_x to projected_x @ W\n",
        "\n",
        "    elif which_phi == 'performer':\n",
        "        def phi(x, c=0):\n",
        "            \"\"\"Uses an exponential random feature map to approximate softmax attention.\"\"\"\n",
        "            # Transpose W to ensure correct dimensions for multiplication\n",
        "            return torch.exp(-0.5 * torch.log(torch.tensor(m, device=device)) + x @ W - 0.5 * (torch.norm(x) ** 2)) # Changed W.T @ x to x @ W\n",
        "\n",
        "    elif which_phi == 'linear':\n",
        "        def phi(x, c=0):\n",
        "            \"\"\"Uses a linear random feature map to approximate softmax attention.\"\"\"\n",
        "            h = -0.5 * torch.log(torch.tensor(m, device=device)) + W @ x - 0.5 * (torch.norm(x) ** 2)\n",
        "            return 1 + h\n",
        "\n",
        "    elif which_phi == 'truncated_performer':\n",
        "        def phi(x, thresh=150):\n",
        "            \"\"\"Uses an exponential random feature map to approximate softmax attention.\"\"\"\n",
        "            scaling_factors = torch.exp(-0.5 * torch.log(torch.tensor(m, device=device)) - 0.5 * (torch.norm(x) ** 2))\n",
        "            h = torch.exp(W @ x) # Multiplication should now work correctly\n",
        "            return scaling_factors * torch.clamp(h, min=0, max=thresh)\n",
        "\n",
        "    elif which_phi == 'positive_cosine':\n",
        "        # Random biases for cosine feature map\n",
        "        rand_b = torch.rand(m, device=device) * 2 * torch.pi\n",
        "\n",
        "        def phi(x, thresh=10):\n",
        "            \"\"\"Uses a positive cosine random feature map to approximate softmax attention.\"\"\"\n",
        "            # Convert m to a tensor to ensure correct type for division\n",
        "            m_tensor = torch.tensor(m, device=device, dtype=torch.float32)\n",
        "            # Project x to dimension D using a linear layer with no bias\n",
        "            projected_x = nn.Linear(x.shape[-1], D, bias=False).to(device)(x)\n",
        "            scaling_factors = torch.sqrt(2.0 / (torch.pi * m_tensor)) * torch.exp(0.5 * (torch.norm(projected_x) ** 2))\n",
        "            h = torch.cos(W @ projected_x + rand_b)  # Use projected_x for matrix multiplication\n",
        "            return torch.clamp(scaling_factors * h, min=0)\n",
        "\n",
        "    elif which_phi == 'dima_sin':\n",
        "        # Random biases for cosine feature map\n",
        "        rand_b = torch.rand(m, device=device) * 2 * torch.pi\n",
        "\n",
        "        def clipped_sin(x):\n",
        "            \"\"\"Clips the sine values.\"\"\"\n",
        "            return torch.where(x > torch.pi / 2, 1, torch.where(x < -torch.pi / 2, -1, torch.sin(x)))\n",
        "\n",
        "        def phi(x, thresh=10):\n",
        "            \"\"\"Uses a sine-based random feature map to approximate softmax attention.\"\"\"\n",
        "            # Convert m to a tensor to ensure correct type for division\n",
        "            m_tensor = torch.tensor(m, device=device, dtype=torch.float32) # Convert m to a tensor\n",
        "\n",
        "            # Project x to dimension D using a linear layer with no bias\n",
        "            projected_x = nn.Linear(x.shape[-1], D, bias=False).to(device)(x)  # Project x to D dimensions\n",
        "\n",
        "            scaling_factors = torch.sqrt(2.0 / m_tensor) * torch.exp(0.5 * (torch.norm(projected_x) ** 2)) # Use m_tensor for division\n",
        "            h = clipped_sin(W @ projected_x + rand_b) # Use projected_x for matrix multiplication\n",
        "            return scaling_factors * h\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown phi type: {which_phi}\")\n",
        "\n",
        "    return phi\n",
        "\n",
        "def get_astro_responses(query_layer, key_layer, nhead, phi):\n",
        "    \"\"\"\n",
        "    Computes astrocyte response given a random feature map, queries, and keys.\n",
        "\n",
        "    Args:\n",
        "        query_layer: Tensor of shape (n_sample, ntokens, dim)\n",
        "        key_layer: Tensor of shape (nhead, ntokens, dim)\n",
        "        nhead: Integer index for the current head\n",
        "        phi: Function to apply to the keys and queries\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape (n_sample, ntokens, ntokens) representing astro_pulses.\n",
        "    \"\"\"\n",
        "    # Get the device of the query_layer\n",
        "    device = query_layer.device\n",
        "\n",
        "    rfa_normalized_keys = phi(key_layer[nhead])\n",
        "    transformed_queries = phi(query_layer)\n",
        "\n",
        "\n",
        "    rfa_normalized_keys = rfa_normalized_keys.T\n",
        "\n",
        "    # Perform batched matrix multiplication\n",
        "    astro_pulses = torch.matmul(transformed_queries, rfa_normalized_keys)\n",
        "\n",
        "    return astro_pulses\n",
        "\n",
        "\n",
        "def neurogenesis(head_size, query_layer, key_layer, nhead, phi='performer'):\n",
        "    # Normalize Q and K matrices appropriately\n",
        "    query_layer = query_layer / head_size ** (1/4)\n",
        "    key_layer = key_layer / head_size ** (1/4)\n",
        "\n",
        "    # Ensure tensors are on GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    query_layer = query_layer.to(device)\n",
        "    key_layer = key_layer.to(device)\n",
        "\n",
        "\n",
        "    phi_low_m = get_phi(m=1024, D=1024, which_phi=phi) #changed value of D from 512 to 1024\n",
        "\n",
        "\n",
        "    astro_ps_low_m = get_astro_responses(query_layer, key_layer, 0, phi_low_m)\n",
        "\n",
        "    # Move the result back to CPU for further processing or conversion to NumPy\n",
        "    return astro_ps_low_m.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "br5BN1V1sKCl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CustomBERTEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, ff_hidden_size, dropout=0.1, neurogenesis=True, phi='performer'):\n",
        "\n",
        "        super(CustomBERTEncoderBlock, self).__init__()\n",
        "        self.query_fc = nn.Linear(embed_size, embed_size)\n",
        "        self.key_fc = nn.Linear(embed_size, embed_size)\n",
        "        self.value_fc = nn.Linear(embed_size, embed_size)\n",
        "        self.attn_out_fc = nn.Linear(embed_size, embed_size)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_size, ff_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.neurogenesis = neurogenesis\n",
        "        self.phi = phi\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.query_fc(x)\n",
        "        key = self.key_fc(x)\n",
        "        value = self.value_fc(x)\n",
        "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
        "        # Apply neurogenesis only if enabled\n",
        "        if self.neurogenesis:\n",
        "            low = neurogenesis(512, query, key, 4, phi=self.phi)\n",
        "            # # Convert low and high to PyTorch tensors before applying softmax\n",
        "            low = torch.tensor(low, device=x.device, dtype=torch.float32)  # Ensure correct data type\n",
        "            attn_weights = F.softmax(low, dim=-1)\n",
        "        else:\n",
        "            attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "\n",
        "        return x, key, query, value\n",
        "\n",
        "\n",
        "class BertForSequenceClassification_Neuro(PreTrainedModel): # Inherit from PreTrainedModel\n",
        "    def __init__(self, config, pretrained_model_name='bert-large-uncased', num_classes=8, ff_hidden_size=2048, dropout=0.1, use_custom_encoder=True, neuro_genesis=True, phi='performer'):\n",
        "\n",
        "        super(BertForSequenceClassification_Neuro, self).__init__(config) # Pass config to super()\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model_name, config=config) # Pass config to BertModel\n",
        "        self.use_custom_encoder = use_custom_encoder\n",
        "        self.num_labels = num_classes\n",
        "        embed_size = self.bert.config.hidden_size\n",
        "\n",
        "        if use_custom_encoder:\n",
        "            self.custom_encoder = CustomBERTEncoderBlock(embed_size, ff_hidden_size, dropout,neurogenesis=neurogenesis, phi=phi)\n",
        "\n",
        "        self.classifier = nn.Linear(embed_size, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        bert_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True\n",
        "        )\n",
        "        sequence_output = bert_output.last_hidden_state\n",
        "        pooled_output = bert_output.pooler_output\n",
        "\n",
        "        if self.use_custom_encoder:\n",
        "            sequence_output, key, query, value = self.custom_encoder(sequence_output)\n",
        "            pooled_output = sequence_output[:, 0, :]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if self.use_custom_encoder:\n",
        "            return logits\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j3Q73TB8sMY2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_json(path):\n",
        "    \"\"\" Read a json file from the given path.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(data, path):\n",
        "    \"\"\" Write a json file to the given path.\"\"\"\n",
        "    if not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "\n",
        "\n",
        "def data_preparation(data):\n",
        "  data_prepared = []\n",
        "  for item in data:\n",
        "\n",
        "    sentence = item[\"sentence\"]\n",
        "    entity1 = item[\"subject\"]\n",
        "    entity2 = item[\"object\"]\n",
        "    relation = item[\"relation\"]\n",
        "    sentence_e1 = sentence.replace(entity1, f\"[E1]{entity1}[/E1]\")\n",
        "    sentence_e2 = sentence_e1.replace(entity2, f\"[E2]{entity2}[/E2]\")\n",
        "    row = {\"sentence\": \"[CLS] \"+sentence_e2, \"relation\": relation, \"e1\":entity1, \"e2\":entity2}\n",
        "    data_prepared.append(row)\n",
        "  return data_prepared\n",
        "\n",
        "\n",
        "class RelationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ws_pw9eP6Msd"
      },
      "outputs": [],
      "source": [
        "class TaskRouter(nn.Module):\n",
        "    def __init__(self, input_dim, num_tasks, pretrained_model=None):\n",
        "        super(TaskRouter, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_tasks)\n",
        "\n",
        "        if pretrained_model:\n",
        "            # Initialize router weights using a pretrained model\n",
        "            with torch.no_grad():\n",
        "                pretrained_weights = pretrained_model.state_dict()\n",
        "                if \"classifier.weight\" in pretrained_weights and \"classifier.bias\" in pretrained_weights:\n",
        "                    self.fc.weight.copy_(pretrained_weights[\"classifier.weight\"][:num_tasks, :input_dim])\n",
        "                    self.fc.bias.copy_(pretrained_weights[\"classifier.bias\"][:num_tasks])\n",
        "                else:\n",
        "                    print(\"Pretrained weights not compatible; using default initialization.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.softmax(self.fc(x), dim=1)  # Probabilities for each task\n",
        "\n",
        "class DynamicMixtureOfTasks(nn.Module):\n",
        "\n",
        "    def __init__(self, initial_tasks, neuro_genesis, neuro_phi, num_classes=4):\n",
        "        super(DynamicMixtureOfTasks, self).__init__()\n",
        "        self.neuro_genesis = neuro_genesis\n",
        "        self.neuro_phi = neuro_phi\n",
        "\n",
        "        config = BertConfig.from_pretrained(\"bert-large-uncased\", num_labels=num_classes)\n",
        "\n",
        "\n",
        "        self.tasks = nn.ModuleList([\n",
        "            BertForSequenceClassification_Neuro(config,\n",
        "                                                pretrained_model_name='bert-large-uncased',\n",
        "                                                num_classes=num_classes,\n",
        "                                                use_custom_encoder=True,\n",
        "                                                neuro_genesis=neuro_genesis,\n",
        "                                                phi=neuro_phi) for _ in range(len(initial_tasks))])\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(\"bert-large-uncased\", config=config)\n",
        "        pretrained_classifier = BertForSequenceClassification.from_pretrained(\n",
        "            \"bert-large-uncased\", num_labels=len(initial_tasks)\n",
        "        )\n",
        "        self.router = TaskRouter(1024, len(initial_tasks), pretrained_classifier)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        router_input = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True\n",
        "        ).pooler_output\n",
        "        task_probs = self.router(router_input)\n",
        "        selected_tasks = torch.argmax(task_probs, dim=1)\n",
        "        batch_outputs = []\n",
        "\n",
        "        # Iterate through each sample in the batch\n",
        "        for i, selected_task in enumerate(selected_tasks):\n",
        "            # Get the output of the selected task model for the current sample\n",
        "            task_output = self.tasks[selected_task](input_ids[i].unsqueeze(0), attention_mask[i].unsqueeze(0))\n",
        "\n",
        "            # Append the task output to the batch_outputs list\n",
        "            batch_outputs.append(task_output)\n",
        "\n",
        "        # Stack the outputs of all task models to form the final output\n",
        "        batch_outputs = torch.cat(batch_outputs, dim=0)\n",
        "\n",
        "        return batch_outputs, task_probs\n",
        "\n",
        "    def add_task(self, num_classes):\n",
        "        config = BertConfig.from_pretrained(\"bert-large-uncased\", num_labels=num_classes)\n",
        "        new_task = BertForSequenceClassification_Neuro(config,\n",
        "                                                pretrained_model_name='bert-large-uncased',\n",
        "                                                num_classes=num_classes,\n",
        "                                                use_custom_encoder=True,\n",
        "                                                neuro_genesis=self.neuro_genesis,\n",
        "                                                phi=self.neuro_phi)\n",
        "        self.tasks.append(new_task)\n",
        "        num_tasks = len(self.tasks)\n",
        "        self.router.fc = nn.Linear(1024, num_tasks)\n",
        "        self.router.bn = nn.BatchNorm1d(num_tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sdirtjfCo6KZ"
      },
      "outputs": [],
      "source": [
        "def compute_task_ids(labels):\n",
        "    task_clusters = {\n",
        "        1: [0, 1, 2, 3],\n",
        "        2: [4, 5, 6, 7],\n",
        "        3: [8, 9, 10, 11],\n",
        "        4: [12, 13, 14, 15],\n",
        "        5: [16, 17, 18, 19],\n",
        "        6: [20, 21, 22, 23],\n",
        "        7: [24, 25, 26, 27],\n",
        "        8: [28, 29, 30, 31],\n",
        "        9: [32, 33, 34, 35],\n",
        "        10: [36, 37, 38, 39]\n",
        "    }\n",
        "    task_ids = []\n",
        "    for key, values in task_clusters.items(): # Unpack the tuple into key, values\n",
        "        for label in labels:\n",
        "            if label in values:\n",
        "                task_ids.append(key) # Append the key to task_ids\n",
        "    return task_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j7cMdRMagPxf"
      },
      "outputs": [],
      "source": [
        "def train_model(epochs, batch_size, val_dataset, train_dataset, model, run_id, task_id, patience=3):\n",
        "    \"\"\"\n",
        "    Trains a BERT-based model on a given dataset with validation, early stopping, and best model loading.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training and validation.\n",
        "        val_dataset: Validation dataset.\n",
        "        train_dataset: Training dataset.\n",
        "        model: Pretrained model to fine-tune.\n",
        "        run_id (str): Unique identifier for the training run.\n",
        "        task_id (str): Task identifier for model saving.\n",
        "        patience (int): Number of epochs to wait for validation loss improvement before stopping.\n",
        "\n",
        "    Returns:\n",
        "        model: The best trained model (based on validation loss).\n",
        "        tokenizer: The tokenizer associated with the model.\n",
        "        train_hist: Training history containing loss, accuracy, and learning rate for each epoch.\n",
        "    \"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "    model = model.to(\"cuda\")\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)  # Adjust learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    train_hist = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    no_improvement_epochs = 0\n",
        "    print(epochs)\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "        for batch in train_loader_tqdm:\n",
        "\n",
        "            task_ids = compute_task_ids(batch['label'])\n",
        "            task_ids_tensor = torch.tensor(task_ids)\n",
        "            task_ids_tensor = task_ids_tensor.to(\"cuda\")\n",
        "\n",
        "            # print(f\"task_ids: {task_ids}\")\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(\"cuda\")\n",
        "            attention_mask = batch['attention_mask'].to(\"cuda\")\n",
        "            labels = batch['label'].to(\"cuda\")\n",
        "\n",
        "            outputs, task_prob_router = model(input_ids, attention_mask)\n",
        "            logits = outputs.squeeze(1)\n",
        "            task_prob_router_logits = task_prob_router\n",
        "            loss_router = criterion(task_prob_router_logits, task_ids_tensor)\n",
        "\n",
        "            model_loss = criterion(logits, labels)\n",
        "\n",
        "            loss = model_loss + loss_router\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            train_loader_tqdm.set_postfix({\"Batch Loss\": loss.item(), \"Router Loss\": loss_router.item(), \"Model Loss\":model_loss.item()})\n",
        "\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        val_router_preds, val_router_labels = [], []\n",
        "        val_loader_tqdm = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader_tqdm:\n",
        "                input_ids = batch['input_ids'].to(\"cuda\")\n",
        "                attention_mask = batch['attention_mask'].to(\"cuda\")\n",
        "                labels = batch['label'].to(\"cuda\")\n",
        "                task_ids = compute_task_ids(batch['label'])\n",
        "                task_ids_tensor = torch.tensor(task_ids)\n",
        "                task_ids_tensor = task_ids_tensor.to(\"cuda\")\n",
        "\n",
        "                outputs, task_prob = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits  = outputs.squeeze(1)\n",
        "                model_loss = criterion(logits, labels)\n",
        "                loss_router = criterion(task_prob, task_ids_tensor)\n",
        "                loss = model_loss + loss_router\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "                val_router_preds.extend(torch.argmax(task_prob, dim=1).cpu().numpy())\n",
        "                val_router_labels.extend(task_ids_tensor.cpu().numpy())\n",
        "\n",
        "                val_loader_tqdm.set_postfix({\"Batch Loss\": loss.item(), \"Router Loss\":loss_router.item(), \"Model Loss\":model_loss.item()})\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "        val_router_accuracy = accuracy_score(val_router_labels, val_router_preds)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f},  Router Val Accuracy = {val_router_accuracy:.4f}, Learning Rate = {scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        epoch_log = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'val_router_accuracy': val_router_accuracy,\n",
        "            'router_loss': loss_router.item(),\n",
        "            'model_loss': model_loss.item(),\n",
        "            'learning_rate': scheduler.optimizer.param_groups[0]['lr']\n",
        "        }\n",
        "        train_hist.append(epoch_log)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            no_improvement_epochs = 0\n",
        "        else:\n",
        "            no_improvement_epochs += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improvement_epochs >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
        "            break\n",
        "\n",
        "    # Load the best model state into the original model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state) # Changed this line\n",
        "        print(\"Loaded the best model based on validation loss.\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model, tokenizer, train_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FY-mC7LegUSB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_model(model, test_dataset, batch_size,run_id, task_id, out_dir):\n",
        "  model.eval()\n",
        "  test_preds, test_labels = [], []\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "  test_loader_tqdm = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "  test_router_preds, test_router_labels = [], []\n",
        "  with torch.no_grad():\n",
        "      for batch in test_loader_tqdm:\n",
        "          input_ids = batch['input_ids'].to(\"cuda\")\n",
        "          attention_mask = batch['attention_mask'].to(\"cuda\")\n",
        "          labels = batch['label'].to(\"cuda\")\n",
        "          outputs, task_prob = model(input_ids=input_ids, attention_mask=attention_mask) # Pass attention_mask to the model\n",
        "          task_ids = compute_task_ids(batch['label'])\n",
        "          task_ids_tensor = torch.tensor(task_ids)\n",
        "          task_ids_tensor = task_ids_tensor.to(\"cuda\")\n",
        "\n",
        "          logits = outputs\n",
        "          test_router_preds.extend(torch.argmax(task_prob, dim=1).cpu().numpy())\n",
        "          test_router_labels.extend(task_ids_tensor.cpu().numpy())\n",
        "          test_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())  # Use logits for argmax\n",
        "          test_labels.extend(labels.cpu().numpy())\n",
        "  # print(test_labels)\n",
        "  # print(test_preds)\n",
        "\n",
        "\n",
        "  test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "  test_router_accuracy = accuracy_score(test_router_labels, test_router_preds)\n",
        "  print(f\"Test Router Accuracy: {test_router_accuracy:.4f}\")\n",
        "\n",
        "  test_preds = [{i:int(pred)} for i, pred in enumerate(test_preds)]\n",
        "  test_labels = [{i:int(label)} for i, label in enumerate(test_labels)]\n",
        "  write_json(test_labels, f\"{out_dir}/test_labels_{run_id}_{task_id}.json\")\n",
        "  write_json(test_preds, f\"{out_dir}/test_preds_{run_id}_{task_id}.json\")\n",
        "\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "  print(f\"Len of Test set: {len(test_preds)}\")\n",
        "  return test_accuracy, test_router_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5BqbgAtfIUG3"
      },
      "outputs": [],
      "source": [
        "def retun_new_task_data(run_id, task_id, dataset_path):\n",
        "\n",
        "\n",
        "    train_data = read_json(f\"{dataset_path}/train/run_{run_id}/task{task_id}/train_1.json\")\n",
        "    val_data = read_json(f\"{dataset_path}/train/run_{run_id}/task{task_id}/dev_1.json\")\n",
        "    test_data = read_json(f\"{dataset_path}/test/run_{run_id}/task{task_id}/test_1.json\")\n",
        "\n",
        "    train_prepared = data_preparation(train_data)\n",
        "    val_prepared = data_preparation(val_data)\n",
        "    test_prepared = data_preparation(test_data)\n",
        "\n",
        "    train_labels = [item['relation'] for item in train_prepared]\n",
        "    val_labels = [item['relation'] for item in val_prepared]\n",
        "    test_labels = [item['relation'] for item in test_prepared]\n",
        "\n",
        "    label_to_int = {label: idx for idx, label in enumerate(set(train_labels))}\n",
        "    train_labels = [label_to_int[label] for label in train_labels]\n",
        "    val_labels = [label_to_int[label] for label in val_labels]\n",
        "    test_labels = [label_to_int[label] for label in test_labels]\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "    max_length = 128\n",
        "    train_dataset = RelationDataset([item['sentence'] for item in train_prepared], train_labels, tokenizer, max_length)\n",
        "    val_dataset = RelationDataset([item['sentence'] for item in val_prepared], val_labels, tokenizer, max_length)\n",
        "    test_dataset = RelationDataset([item['sentence'] for item in test_prepared], test_labels, tokenizer, max_length)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, label_to_int\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkmDy2pu9-s1",
        "outputId": "ceee823f-14ea-48ef-b77a-6760c5c6db9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(all_train_dataset): 2984\n",
            "5\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.6679, Val Loss = 0.4461, Val Accuracy = 0.9495, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.3930, Val Loss = 0.6713, Val Accuracy = 0.9170, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.3753, Val Loss = 0.5400, Val Accuracy = 0.9495, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.3715, Val Loss = 0.6042, Val Accuracy = 0.9206, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000002\n",
            "Early stopping triggered after 4 epochs.\n",
            "Loaded the best model based on validation loss.\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Router Accuracy: 1.0000\n",
            "Test Accuracy: 0.9572\n",
            "Len of Test set: 257\n",
            "len(model.tasks): 3\n",
            "len(all_train_dataset): 1482\n",
            "5\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.9055, Val Loss = 0.9786, Val Accuracy = 0.8446, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.5980, Val Loss = 0.9696, Val Accuracy = 0.8784, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.5726, Val Loss = 1.2472, Val Accuracy = 0.8514, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.5734, Val Loss = 1.0981, Val Accuracy = 0.8514, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.6237, Val Loss = 0.9958, Val Accuracy = 0.8851, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000002\n",
            "Early stopping triggered after 5 epochs.\n",
            "Loaded the best model based on validation loss.\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Router Accuracy: 1.0000\n",
            "Test Accuracy: 0.8015\n",
            "Len of Test set: 388\n",
            "len(model.tasks): 4\n",
            "len(all_train_dataset): 1976\n",
            "5\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.8946, Val Loss = 0.9007, Val Accuracy = 0.9500, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.7661, Val Loss = 0.8750, Val Accuracy = 0.9714, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.7728, Val Loss = 0.8534, Val Accuracy = 0.9786, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.7804, Val Loss = 0.8931, Val Accuracy = 0.9500, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.7483, Val Loss = 0.8129, Val Accuracy = 0.9857, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Loaded the best model based on validation loss.\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Router Accuracy: 1.0000\n",
            "Test Accuracy: 0.6433\n",
            "Len of Test set: 513\n",
            "len(model.tasks): 5\n",
            "len(all_train_dataset): 1436\n",
            "5\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.1227, Val Loss = 0.9953, Val Accuracy = 0.9735, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.9097, Val Loss = 1.0811, Val Accuracy = 0.9603, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.9050, Val Loss = 1.0493, Val Accuracy = 0.9801, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 0.9049, Val Loss = 1.0606, Val Accuracy = 0.9801, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000002\n",
            "Early stopping triggered after 4 epochs.\n",
            "Loaded the best model based on validation loss.\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Router Accuracy: 1.0000\n",
            "Test Accuracy: 0.3146\n",
            "Len of Test set: 639\n",
            "len(model.tasks): 6\n",
            "len(all_train_dataset): 1440\n",
            "5\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.5477, Val Loss = 1.4093, Val Accuracy = 0.9154, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.1354, Val Loss = 1.4358, Val Accuracy = 0.9231, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.0544, Val Loss = 1.4484, Val Accuracy = 0.9308, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.0439, Val Loss = 1.4816, Val Accuracy = 0.9231, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000002\n",
            "Early stopping triggered after 4 epochs.\n",
            "Loaded the best model based on validation loss.\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Router Accuracy: 1.0000\n",
            "Test Accuracy: 0.3720\n",
            "Len of Test set: 750\n",
            "len(model.tasks): 7\n",
            "len(all_train_dataset): 1318\n",
            "5\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.4076, Val Loss = 1.2289, Val Accuracy = 0.9726, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.1733, Val Loss = 1.2244, Val Accuracy = 0.9726, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.1655, Val Loss = 1.2301, Val Accuracy = 0.9726, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.1654, Val Loss = 1.2322, Val Accuracy = 0.9726, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.1654, Val Loss = 1.2334, Val Accuracy = 0.9726, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000002\n",
            "Early stopping triggered after 5 epochs.\n",
            "Loaded the best model based on validation loss.\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Router Accuracy: 1.0000\n",
            "Test Accuracy: 0.4328\n",
            "Len of Test set: 878\n",
            "len(model.tasks): 8\n",
            "len(all_train_dataset): 926\n",
            "5\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.6044, Val Loss = 1.6501, Val Accuracy = 0.8503, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.3075, Val Loss = 1.6016, Val Accuracy = 0.9048, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.2748, Val Loss = 1.6232, Val Accuracy = 0.8844, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.2742, Val Loss = 1.6462, Val Accuracy = 0.8912, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.2741, Val Loss = 1.6747, Val Accuracy = 0.8912, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000002\n",
            "Early stopping triggered after 5 epochs.\n",
            "Loaded the best model based on validation loss.\n",
            "Training complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Router Accuracy: 1.0000\n",
            "Test Accuracy: 0.4402\n",
            "Len of Test set: 979\n",
            "len(model.tasks): 9\n",
            "len(all_train_dataset): 1260\n",
            "5\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.5287, Val Loss = 1.3788, Val Accuracy = 1.0000, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.3798, Val Loss = 1.4172, Val Accuracy = 0.9869, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss = 1.3866, Val Loss = 1.4016, Val Accuracy = 0.9935, Router Loss= 1.0000, Router Val Accuracy = 1.0000, Learning Rate = 0.000020\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:  88%| | 68/77 [00:16<00:02,  4.04it/s, Batch Loss=1.37, Router Loss=1.37, Model Loss=4.33e-5]"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def main(out_dir, dataset_path, neuro_genesis, baseline_name, epoch_list, batch_list, neuro_phi):\n",
        "\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "    for run_id in range(1,2):\n",
        "        torch.cuda.empty_cache()\n",
        "        results = []\n",
        "        all_train_texts, all_train_labels = [], []  # Initialize empty lists for texts and labels\n",
        "        all_val_texts, all_val_labels = [], []\n",
        "        all_test_texts, all_test_labels = [], []\n",
        "\n",
        "        initial_task_id = 1\n",
        "        model = DynamicMixtureOfTasks(\n",
        "            initial_tasks=[4,4],\n",
        "            neuro_genesis=neuro_genesis,\n",
        "            neuro_phi=neuro_phi,\n",
        "            num_classes=4\n",
        "        )\n",
        "        train_dataset, val_dataset, test_dataset, label_to_int = retun_new_task_data(run_id, initial_task_id, dataset_path)\n",
        "\n",
        "        # Get the texts and labels from the initial dataset\n",
        "        all_train_texts.extend(train_dataset.texts)\n",
        "        all_train_labels.extend(train_dataset.labels)  # Use the labels attribute\n",
        "        all_val_texts.extend(val_dataset.texts)  # Access the texts attribute for val_dataset\n",
        "        all_val_labels.extend(val_dataset.labels)\n",
        "        all_test_texts.extend(test_dataset.texts)  # Access the texts attribute for test_dataset\n",
        "        all_test_labels.extend(test_dataset.labels)\n",
        "\n",
        "        for task_id in range(2, 11):\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            train_dataset, val_dataset, test_dataset, label_to_int = retun_new_task_data(run_id, task_id, dataset_path)\n",
        "\n",
        "            # Extend the combined lists with data from the current task\n",
        "            all_train_texts.extend(train_dataset.texts)  # Access texts attribute\n",
        "            all_train_labels.extend(train_dataset.labels)\n",
        "            all_val_texts.extend(val_dataset.texts)  # Access texts attribute\n",
        "            all_val_labels.extend(val_dataset.labels)\n",
        "            all_test_texts.extend(test_dataset.texts)  # Access texts attribute\n",
        "            all_test_labels.extend(test_dataset.labels)\n",
        "\n",
        "            print(f\"len(all_train_dataset): {len(all_train_texts)}\") # Print the length of the combined dataset\n",
        "\n",
        "            for i, epoch in enumerate(epoch_list):\n",
        "\n",
        "                for batch in batch_list:\n",
        "                    train_dataset = RelationDataset([item for item in all_train_texts], all_train_labels, tokenizer, 512)\n",
        "                    val_dataset = RelationDataset([item for item in all_val_texts], all_val_labels, tokenizer, 512)\n",
        "                    test_dataset = RelationDataset([item for item in all_test_texts], all_test_labels, tokenizer, 512)\n",
        "                    model, tokenizer, train_hist = train_model(epoch, batch, val_dataset, train_dataset, model, run_id, task_id)\n",
        "\n",
        "                    write_json(train_hist, f\"{out_dir}/train_hist_{run_id}_{task_id}.json\")\n",
        "                    # model_name_hf = f\"{baseline_name}_{run_id}_{task_id}\"\n",
        "                    # base_model_hf = f\"Sefika/bert_large_baseline_{run_id}_{task_id}\"\n",
        "                    # model.push_to_hub(model_name_hf, private=True)\n",
        "                    test_acc, test_router_acc = test_model(model, test_dataset, batch,run_id, task_id, out_dir)\n",
        "                    # test_seen_acc = test_model(model, all_seen_test_data, batch)\n",
        "                    result = {\"run_id\":run_id, \"task_id\": task_id, \"epoch\": epoch, \"batch_size\": batch, \"test_acc\": test_acc, \"test_router_acc\":test_router_acc}\n",
        "                    results.append(result)\n",
        "                    write_json(results, f\"{out_dir}/results_{run_id}.json\")\n",
        "\n",
        "                    torch.cuda.empty_cache()\n",
        "            all_train_texts, all_train_labels = [], []  # Initialize empty lists for texts and labels\n",
        "            all_val_texts, all_val_labels = [], []\n",
        "            # all_test_texts, all_test_labels = [], []\n",
        "            if task_id != 10:\n",
        "              model.add_task(len(label_to_int))\n",
        "              print(f\"len(model.tasks): {len(model.tasks)}\")\n",
        "\n",
        "                # del model\n",
        "    del tokenizer\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #drive folde = EMNLP-neurogenesis\n",
        "\n",
        "    # phi = ['cosine','performer', 'linear', 'truncated_performer', 'positive_cosine', 'dima_sin']\n",
        "    phi = ['performer']\n",
        "    neuro_genesis = True\n",
        "    epoch_list = [5]\n",
        "    batch_list = [4]\n",
        "    for neuro_genesis_type_phi in phi:\n",
        "      baseline_name = \"bert_large_performer_neurogenesis\"\n",
        "      dataset_path = \"/content/tacred/final\"\n",
        "      output_dir = f\"drive/MyDrive/EMNLP-neurogenesis/mix_of_tasks\"\n",
        "\n",
        "      main(output_dir, dataset_path, neuro_genesis, baseline_name, epoch_list, batch_list, neuro_genesis_type_phi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqLxJ2QuNCrO",
        "outputId": "215ac7c0-e9da-4234-a7b6-2843c3687b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BNbbPmr_IF8F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1BEE9RkseYp"
      },
      "source": [
        "## Mix of Tasks Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np9nUvXkuO_k"
      },
      "source": [
        "task_labels"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xGykAavP9S83"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
