{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv05IE3jiHC1"
      },
      "outputs": [],
      "source": [
        "!unzip tacred.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/trl\n",
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "id": "VMxybHHyjGP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token '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'"
      ],
      "metadata": {
        "id": "NSvzl9MpjLsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "7ZcQ_8FliOl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, PreTrainedModel, BertTokenizer, BertConfig\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "VXq2QKl1ipFK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### source code"
      ],
      "metadata": {
        "id": "P4BBPQymirie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_phi(m, D, which_phi='performer', device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"\n",
        "    Function that returns the random feature map, phi.\n",
        "    Since our neuron-astrocyte model is equivalent to using Random Feature Attention,\n",
        "    we use this representation for simplicity. Different phi functions lead to different feature maps.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Random weight matrix for random feature map\n",
        "    W = torch.randn((m, D), device=device)\n",
        "\n",
        "    if which_phi == 'cosine':\n",
        "        # Random biases for cosine feature map\n",
        "        rand_b = torch.rand(m, device=device) * 2 * torch.pi\n",
        "\n",
        "        def phi(x, c=0):\n",
        "            \"\"\"Uses a cosine random feature map to approximate softmax attention.\"\"\"\n",
        "            return torch.sqrt(2 / m) * torch.cos(W @ x + rand_b) * torch.exp(0.5 * (torch.norm(x) ** 2) - c)\n",
        "\n",
        "    elif which_phi == 'performer':\n",
        "        def phi(x, c=0):\n",
        "            \"\"\"Uses an exponential random feature map to approximate softmax attention.\"\"\"\n",
        "            return torch.exp(-0.5 * torch.log(torch.tensor(m, device=device)) + W @ x - 0.5 * (torch.norm(x) ** 2))\n",
        "\n",
        "    elif which_phi == 'linear':\n",
        "        def phi(x, c=0):\n",
        "            \"\"\"Uses a linear random feature map to approximate softmax attention.\"\"\"\n",
        "            h = -0.5 * torch.log(torch.tensor(m, device=device)) + W @ x - 0.5 * (torch.norm(x) ** 2)\n",
        "            return 1 + h\n",
        "\n",
        "    elif which_phi == 'truncated_performer':\n",
        "        def phi(x, thresh=150):\n",
        "            \"\"\"Uses an exponential random feature map to approximate softmax attention.\"\"\"\n",
        "            scaling_factors = torch.exp(-0.5 * torch.log(torch.tensor(m, device=device)) - 0.5 * (torch.norm(x) ** 2))\n",
        "            h = torch.exp(W @ x)\n",
        "            return scaling_factors * torch.clamp(h, min=0, max=thresh)\n",
        "\n",
        "    elif which_phi == 'positive_cosine':\n",
        "        # Random biases for cosine feature map\n",
        "        rand_b = torch.rand(m, device=device) * 2 * torch.pi\n",
        "\n",
        "        def phi(x, thresh=10):\n",
        "            \"\"\"Uses a positive cosine random feature map to approximate softmax attention.\"\"\"\n",
        "            scaling_factors = torch.sqrt(2 / (torch.pi * m)) * torch.exp(0.5 * (torch.norm(x) ** 2))\n",
        "            h = torch.cos(W @ x + rand_b)\n",
        "            return torch.clamp(scaling_factors * h, min=0)\n",
        "\n",
        "    elif which_phi == 'dima_sin':\n",
        "        # Random biases for cosine feature map\n",
        "        rand_b = torch.rand(m, device=device) * 2 * torch.pi\n",
        "\n",
        "        def clipped_sin(x):\n",
        "            \"\"\"Clips the sine values.\"\"\"\n",
        "            return torch.where(x > torch.pi / 2, 1, torch.where(x < -torch.pi / 2, -1, torch.sin(x)))\n",
        "\n",
        "        def phi(x, thresh=10):\n",
        "            \"\"\"Uses a sine-based random feature map to approximate softmax attention.\"\"\"\n",
        "            scaling_factors = torch.sqrt(2 / m) * torch.exp(0.5 * (torch.norm(x) ** 2))\n",
        "            h = clipped_sin(W @ x + rand_b)\n",
        "            return scaling_factors * h\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown phi type: {which_phi}\")\n",
        "\n",
        "    return phi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_astro_responses(query_layer, key_layer, nhead, phi):\n",
        "    \"\"\"\n",
        "    Computes astrocyte response given a random feature map, queries, and keys.\n",
        "\n",
        "    Args:\n",
        "        query_layer: Tensor of shape (n_sample, ntokens, dim)\n",
        "        key_layer: Tensor of shape (nhead, ntokens, dim)\n",
        "        nhead: Integer index for the current head\n",
        "        phi: Function to apply to the keys and queries\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape (n_sample, ntokens, ntokens) representing astro_pulses.\n",
        "    \"\"\"\n",
        "    # Get the device of the query_layer\n",
        "    device = query_layer.device\n",
        "\n",
        "    rfa_normalized_keys = phi(key_layer[nhead])\n",
        "    transformed_queries = phi(query_layer)\n",
        "\n",
        "\n",
        "    rfa_normalized_keys = rfa_normalized_keys.T\n",
        "\n",
        "    # Perform batched matrix multiplication\n",
        "    astro_pulses = torch.matmul(transformed_queries, rfa_normalized_keys)\n",
        "\n",
        "    return astro_pulses\n",
        "\n",
        "\n",
        "def neurogenesis(head_size, query_layer, key_layer, nhead, phi='performer'):\n",
        "    # Normalize Q and K matrices appropriately\n",
        "    query_layer = query_layer / head_size ** (1/4)\n",
        "    key_layer = key_layer / head_size ** (1/4)\n",
        "\n",
        "    # Ensure tensors are on GPU\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    query_layer = query_layer.to(device)\n",
        "    key_layer = key_layer.to(device)\n",
        "\n",
        "\n",
        "    phi_low_m = get_phi(m=512, D=head_size, which_phi=phi)\n",
        "\n",
        "\n",
        "    astro_ps_low_m = get_astro_responses(query_layer, key_layer, 0, phi_low_m)\n",
        "\n",
        "    # Move the result back to CPU for further processing or conversion to NumPy\n",
        "    return astro_ps_low_m.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "pZZCboo-iOEy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CustomBERTEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, ff_hidden_size, dropout=0.1, neurogenesis=True, phi='performer'):\n",
        "\n",
        "        super(CustomBERTEncoderBlock, self).__init__()\n",
        "        self.query_fc = nn.Linear(embed_size, embed_size)\n",
        "        self.key_fc = nn.Linear(embed_size, embed_size)\n",
        "        self.value_fc = nn.Linear(embed_size, embed_size)\n",
        "        self.attn_out_fc = nn.Linear(embed_size, embed_size)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_size, ff_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.neurogenesis = neurogenesis\n",
        "        self.phi = phi\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.query_fc(x)\n",
        "        key = self.key_fc(x)\n",
        "        value = self.value_fc(x)\n",
        "        attn_scores = torch.matmul(query, key.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
        "        # Apply neurogenesis only if enabled\n",
        "        if self.neurogenesis:\n",
        "            low = neurogenesis(512, query, key, 4, phi=self.phi)\n",
        "            # # Convert low and high to PyTorch tensors before applying softmax\n",
        "            low = torch.tensor(low, device=x.device, dtype=torch.float32)  # Ensure correct data type\n",
        "            attn_weights = F.softmax(low, dim=-1)\n",
        "        else:\n",
        "            attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "\n",
        "        return x, key, query, value\n",
        "\n",
        "\n",
        "class BertForSequenceClassification_Neuro(PreTrainedModel): # Inherit from PreTrainedModel\n",
        "    def __init__(self, config, pretrained_model_name='bert-large-uncased', num_classes=8, ff_hidden_size=2048, dropout=0.1, use_custom_encoder=True, neuro_genesis=True, phi='performer'):\n",
        "\n",
        "        super(BertForSequenceClassification_Neuro, self).__init__(config) # Pass config to super()\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model_name, config=config) # Pass config to BertModel\n",
        "        self.use_custom_encoder = use_custom_encoder\n",
        "        self.num_labels = num_classes\n",
        "        embed_size = self.bert.config.hidden_size\n",
        "\n",
        "        if use_custom_encoder:\n",
        "            self.custom_encoder = CustomBERTEncoderBlock(embed_size, ff_hidden_size, dropout,neurogenesis=neurogenesis, phi=phi)\n",
        "\n",
        "        self.classifier = nn.Linear(embed_size, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        bert_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True\n",
        "        )\n",
        "        sequence_output = bert_output.last_hidden_state\n",
        "        pooled_output = bert_output.pooler_output\n",
        "\n",
        "        if self.use_custom_encoder:\n",
        "            sequence_output, key, query, value = self.custom_encoder(sequence_output)\n",
        "            pooled_output = sequence_output[:, 0, :]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if self.use_custom_encoder:\n",
        "            return logits\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "2PvwyrNyiWm4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_json(path):\n",
        "    \"\"\" Read a json file from the given path.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(data, path):\n",
        "    \"\"\" Write a json file to the given path.\"\"\"\n",
        "    if not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "\n",
        "\n",
        "def data_preparation(data):\n",
        "  data_prepared = []\n",
        "  for item in data:\n",
        "\n",
        "    sentence = item[\"sentence\"]\n",
        "    entity1 = item[\"subject\"]\n",
        "    entity2 = item[\"object\"]\n",
        "    relation = item[\"relation\"]\n",
        "    sentence_e1 = sentence.replace(entity1, f\"[E1]{entity1}[/E1]\")\n",
        "    sentence_e2 = sentence_e1.replace(entity2, f\"[E2]{entity2}[/E2]\")\n",
        "    row = {\"sentence\": \"[CLS] \"+sentence_e2, \"relation\": relation, \"e1\":entity1, \"e2\":entity2}\n",
        "    data_prepared.append(row)\n",
        "  return data_prepared\n",
        "\n",
        "\n",
        "class RelationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "raonoMnyiaAE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model(epochs, batch_size, val_dataset, train_dataset, model, run_id, task_id, patience=3):\n",
        "    \"\"\"\n",
        "    Trains a BERT-based model on a given dataset with validation, early stopping, and best model loading.\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training and validation.\n",
        "        val_dataset: Validation dataset.\n",
        "        train_dataset: Training dataset.\n",
        "        model: Pretrained model to fine-tune.\n",
        "        run_id (str): Unique identifier for the training run.\n",
        "        task_id (str): Task identifier for model saving.\n",
        "        patience (int): Number of epochs to wait for validation loss improvement before stopping.\n",
        "\n",
        "    Returns:\n",
        "        model: The best trained model (based on validation loss).\n",
        "        tokenizer: The tokenizer associated with the model.\n",
        "        train_hist: Training history containing loss, accuracy, and learning rate for each epoch.\n",
        "    \"\"\"\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "    model = model.to(\"cuda\")\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # Adjust learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    train_hist = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    no_improvement_epochs = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "        for batch in train_loader_tqdm:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(\"cuda\")\n",
        "            attention_mask = batch['attention_mask'].to(\"cuda\")\n",
        "            labels = batch['label'].to(\"cuda\")\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            logits = outputs\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            train_loader_tqdm.set_postfix({\"Batch Loss\": loss.item()})\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        val_loader_tqdm = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader_tqdm:\n",
        "                input_ids = batch['input_ids'].to(\"cuda\")\n",
        "                attention_mask = batch['attention_mask'].to(\"cuda\")\n",
        "                labels = batch['label'].to(\"cuda\")\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs\n",
        "                loss = criterion(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                val_loader_tqdm.set_postfix({\"Batch Loss\": loss.item()})\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}, Learning Rate = {scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        epoch_log = {\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'learning_rate': scheduler.optimizer.param_groups[0]['lr']\n",
        "        }\n",
        "        train_hist.append(epoch_log)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            no_improvement_epochs = 0\n",
        "        else:\n",
        "            no_improvement_epochs += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improvement_epochs >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
        "            break\n",
        "\n",
        "    # Load the best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(\"Loaded the best model based on validation loss.\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model, tokenizer, train_hist\n"
      ],
      "metadata": {
        "id": "mP42ghUoid4e"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_model(model, test_dataset, batch_size):\n",
        "  model.eval()\n",
        "  test_preds, test_labels = [], []\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "  test_loader_tqdm = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "  with torch.no_grad():\n",
        "      for batch in test_loader_tqdm:\n",
        "          input_ids = batch['input_ids'].to(\"cuda\")\n",
        "          attention_mask = batch['attention_mask'].to(\"cuda\")\n",
        "          labels = batch['label'].to(\"cuda\")\n",
        "          outputs = model(input_ids=input_ids, attention_mask=attention_mask) # Pass attention_mask to the model\n",
        "\n",
        "          logits = outputs\n",
        "          test_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())  # Use logits for argmax\n",
        "          test_labels.extend(labels.cpu().numpy())\n",
        "  print(test_labels)\n",
        "  print(test_preds)\n",
        "  test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "  return test_accuracy"
      ],
      "metadata": {
        "id": "5cj6w7XxiiW-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def main(out_dir, dataset_path, neuro_genesis, baseline_name,  batch_list, epoch_list, neuro_phi):\n",
        "\n",
        "\n",
        "    # torch.cuda.empty_cache()\n",
        "    for run_id in range(1,6):\n",
        "        results = []\n",
        "\n",
        "        # all_labels = []\n",
        "        # all_seen_test_data = []\n",
        "        # all_test_sentences = []\n",
        "        # all_test_labels = []\n",
        "\n",
        "        for task_id in range(1, 11):\n",
        "\n",
        "            train_data = read_json(f\"{dataset_path}/train/run_{run_id}/task{task_id}/train_1.json\")\n",
        "            train_prepared = data_preparation(train_data)\n",
        "            test_data = read_json(f\"{dataset_path}/test/run_{run_id}/task{task_id}/test_1.json\")\n",
        "            test_prepared = data_preparation(test_data)\n",
        "            val_data = read_json(f\"{dataset_path}/train/run_{run_id}/task{task_id}/dev_1.json\")\n",
        "            val_prepared = data_preparation(val_data)\n",
        "            # all_labels.extend([item['relation'] for item in train_prepared])\n",
        "            # Create unique mapping for all labels\n",
        "\n",
        "\n",
        "            train_labels = [item['relation'] for item in train_prepared]\n",
        "            train_sentences = [item['sentence'] for item in train_prepared]\n",
        "            test_labels = [item['relation'] for item in test_prepared]\n",
        "            test_sentences = [item['sentence'] for item in test_prepared]\n",
        "            val_labels = [item['relation'] for item in val_prepared]\n",
        "            val_sentences = [item['sentence'] for item in val_prepared]\n",
        "            label_to_int = {label: idx for idx, label in enumerate(set(train_labels))}\n",
        "            # all_test_sentences.extend(test_sentences)\n",
        "            # all_test_labels.extend(test_labels)\n",
        "\n",
        "            # # Convert labels to integers using the pre-calculated mapping\n",
        "            train_labels = [label_to_int[label] for label in train_labels]\n",
        "            val_labels = [label_to_int[label] for label in val_labels]\n",
        "            test_labels = [label_to_int[label] for label in test_labels]\n",
        "\n",
        "            # test_labels_seen = [label_to_int[label] for label in all_test_labels]\n",
        "\n",
        "            # Create Dataset and DataLoader\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "\n",
        "\n",
        "                # Load the configuration first\n",
        "            config = BertConfig.from_pretrained('bert-large-uncased', num_labels=len(label_to_int))\n",
        "            # Then pass it to the model constructor\n",
        "            model = BertForSequenceClassification_Neuro(config, pretrained_model_name='bert-large-uncased', num_classes=len(label_to_int), use_custom_encoder=True, neuro_genesis=neuro_genesis, phi=neuro_phi)\n",
        "            # else:\n",
        "\n",
        "            #     #The old model should have the total number of labels encountered till the last task.\n",
        "            #     old_labels = model.num_labels\n",
        "            #     print(f\"Old labels: {old_labels}\")\n",
        "            #     model = increment_class_labels(model, new_num_labels=len(label_to_int)) #Use total number of labels for current task\n",
        "\n",
        "            train_dataset = RelationDataset(train_sentences, train_labels, tokenizer, max_length=512)\n",
        "            val_dataset = RelationDataset(val_sentences, val_labels, tokenizer, max_length=512)\n",
        "\n",
        "            test_dataset = RelationDataset(test_sentences, test_labels, tokenizer, max_length=512)\n",
        "            # all_seen_test_data = RelationDataset(all_test_sentences, test_labels_seen, tokenizer, max_length=512)\n",
        "\n",
        "            for i, epoch in enumerate(epoch_list):\n",
        "\n",
        "                for batch in batch_list:\n",
        "                    model, tokenizer, train_hist = train_model(epoch, batch, val_dataset, train_dataset, model, run_id, task_id)\n",
        "\n",
        "                    write_json(train_hist, f\"{out_dir}/train_hist_{run_id}_{task_id}.json\")\n",
        "                    model_name_hf = f\"{baseline_name}_{run_id}_{task_id}\"\n",
        "                    # base_model_hf = f\"Sefika/bert_large_baseline_{run_id}_{task_id}\"\n",
        "                    model.push_to_hub(model_name_hf, private=True)\n",
        "                    test_acc = test_model(model, test_dataset, batch)\n",
        "                    # test_seen_acc = test_model(model, all_seen_test_data, batch)\n",
        "\n",
        "                    print(f\"Epoch-------------{epoch}=={i}\")\n",
        "                    result = {\"run_id\":run_id, \"task_id\": task_id, \"epoch\": epoch, \"batch_size\": batch, \"test_acc\": test_acc}\n",
        "                    results.append(result)\n",
        "                    write_json(results, f\"{out_dir}/results_{run_id}.json\")\n",
        "\n",
        "                    torch.cuda.empty_cache()\n",
        "                # del model\n",
        "        del tokenizer\n",
        "        del model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    output_dir = \"./neurogenesis_results_low\"\n",
        "    neuro_genesis = True\n",
        "    epoch_list = [1]\n",
        "    batch_list = [16]\n",
        "    baseline_name = \"bert_large_performer_neurogenesis\"\n",
        "    dataset_path = \"/content/tacred/final\"\n",
        "    neuro_genesis_type_phi = \"performer\"\n",
        "    main(output_dir, dataset_path, neuro_genesis,baseline_name, epoch_list, batch_list, neuro_genesis_type_phi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJX5tb1IikvB",
        "outputId": "ba3f8114-6d70-4b10-802e-0da400b2f3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|████▊     | 882/1820 [01:46<02:00,  7.82it/s, Batch Loss=1.03]"
          ]
        }
      ]
    }
  ]
}