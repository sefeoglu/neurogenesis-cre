{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sefeoglu/neurogenesis-cre/blob/master/FS_CRE_neurogenesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0c-IPhqs6f-",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade pip\n",
        "!pip3 install --upgrade transformers\n",
        "!pip3 install --upgrade accelerate\n",
        "!pip3 install sentencepiece\n",
        "!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr --upgrade\n",
        "!pip install ipywidgets\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "!pip install evaluate\n",
        "!pip install trl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dye4g2vFZmop",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !pip uninstall transformers\n",
        "!pip install transformers==4.45.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncY-1M2DXVPu",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !unzip relations.zip\n",
        "!unzip 5way5shot_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTuKyhYz0_zR"
      },
      "outputs": [],
      "source": [
        "!unzip relations.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Continuous instruction fine-tuning of Flan T5 model for Relation Extraction\"\"\"\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from random import randrange\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments\n",
        "from peft import  get_peft_model, LoraConfig, TaskType\n",
        "from transformers import BitsAndBytesConfig\n",
        "from huggingface_hub import HfFolder\n",
        "import evaluate\n",
        "import nltk, torch\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Metric\n",
        "metric = evaluate.load(\"rouge\")\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "from datasets import concatenate_datasets\n"
      ],
      "metadata": {
        "id": "fZSGvj_AiUpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Seq2SeqTrainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
        "        return loss # Added return statement to return the calculated loss"
      ],
      "metadata": {
        "id": "T3UO5m2ZiuzM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xF3bt_RcT7Hg"
      },
      "outputs": [],
      "source": [
        "def read_json(path):\n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def set_dataset(dataset_id):\n",
        "    # Load dataset\n",
        "\n",
        "    dataset = load_dataset(dataset_id)\n",
        "\n",
        "    print(len(dataset['validation']))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def set_tokenizer(model_id=\"google/flan-t5-base\"):\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "tokenizer = set_tokenizer()\n",
        "def preprocess_function(sample,padding=\"max_length\"):\n",
        "    # add prefix to the input for t5\n",
        "    inputs = [item for item in sample[\"prompt\"]]\n",
        "\n",
        "    # tokenize inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `text_target` keyword argument\n",
        "    labels = tokenizer(text_target=sample[\"relation\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# huggingface hub model id\n",
        "def load_model(model_id=\"google/flan-t5-base\", local=False):\n",
        "\n",
        "    compute_dtype = getattr(torch, \"float16\")\n",
        "    # load model from the hub\n",
        "    # maxmem={i:f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB' for i in range()}\n",
        "    # maxmem['cpu']='300GB'\n",
        "    bnb_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_use_double_quant=False,\n",
        "    )\n",
        "\n",
        "    # model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  device_map=\"auto\", max_memory=maxmem, quantization_config=bnb_config)\n",
        "    if local:\n",
        "      model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  device_map=\"auto\", quantization_config=bnb_config, local_files_only=True)\n",
        "    else:\n",
        "      model = AutoModelForSeq2SeqLM.from_pretrained(model_id,  device_map=\"auto\", quantization_config=bnb_config)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "  if isinstance(logits, tuple):\n",
        "    logits = logits[0]\n",
        "\n",
        "  return logits.argmax(dim=-1)\n",
        "# helper function to postprocess text\n",
        "def postprocess_text(labels, preds):\n",
        "    preds = [pred.replace('\\n','').split('Answer:')[-1].strip() for pred in preds]\n",
        "    labels = [label.replace('\\n','').split('Answer:')[-1].strip() for label in labels]\n",
        "    #print(preds)\n",
        "    #print(labels)\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    targets_labels = read_json(tasks_path)\n",
        "    # Replace -100 in the preds as we can't decode them\n",
        "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "\n",
        "    # Decode generated summaries into text\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    # Decode reference summaries into text\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # ROUGE expects a newline after each sentence\n",
        "        # Some simple post-processing\n",
        "\n",
        "    # model_predictions.extend(decoded_preds)\n",
        "    grounds, preds = postprocess_text(decoded_labels,decoded_preds)\n",
        "    tf = [True  if preds[i] == grounds[i] else False for i in range(len(preds))]\n",
        "    print(preds)\n",
        "    print(grounds)\n",
        "    p, r, f, _ = precision_recall_fscore_support(grounds, preds, labels=targets_labels, average='micro')\n",
        "    acc = accuracy_score(grounds, preds)\n",
        "    decoded_preds = [\"\\n\".join(pred.strip()) for pred in decoded_preds]\n",
        "\n",
        "    decoded_labels = [\"\\n\".join(label.strip()) for label in decoded_labels]\n",
        "    # Compute ROUGscores\n",
        "    result = metric.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "    # Extract the median scores\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "\n",
        "\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "def main(model_id, data_path, tasks_path, task_id, local):\n",
        "\n",
        "    targets_labels = read_json(tasks_path)\n",
        "    print(targets_labels)\n",
        "\n",
        "\n",
        "\n",
        "    dataset = set_dataset(data_path)\n",
        "    model = load_model(model_id, local)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    tokenizer = set_tokenizer(model_id=\"google/flan-t5-base\")\n",
        "\n",
        "    # The maximum total input sequence length after tokenization.\n",
        "    # Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "    tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(lambda x: tokenizer(x[\"prompt\"], truncation=True), batched=True, remove_columns=[\"prompt\", \"relation\"])\n",
        "    max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
        "    print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "    # The maximum total sequence length for target text after tokenization.\n",
        "    # Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "    tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(lambda x: tokenizer(x[\"relation\"], truncation=True), batched=True, remove_columns=[\"prompt\", \"relation\"])\n",
        "    max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n",
        "    print(f\"Max target length: {max_target_length}\")\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"prompt\", \"relation\"])\n",
        "    print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
        "\n",
        "    # model = load_model(model_id, local)\n",
        "    # model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "\n",
        "    # we want to ignore tokenizer pad token in the loss\n",
        "    label_pad_token_id = -100\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8\n",
        "    )\n",
        "\n",
        "\n",
        "    # Hugging Face repository id\n",
        "    repository_id = f\"{model_id}-{task_id}\"\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=repository_id,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        predict_with_generate=True,\n",
        "        fp16=False, # Overflows with fp16\n",
        "        learning_rate=1e-3,\n",
        "        num_train_epochs=1,\n",
        "        do_eval=True,\n",
        "        # logging & evaluation strategies\n",
        "        logging_dir=f\"{repository_id}/logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=500,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        # push to hub parameters\n",
        "        report_to=\"tensorboard\",\n",
        "        push_to_hub=False,\n",
        "        hub_strategy=\"every_save\",\n",
        "        hub_model_id=repository_id,\n",
        "        hub_token=HfFolder.get_token(),\n",
        "        lr_scheduler_type = \"cosine_with_restarts\",\n",
        "        lr_scheduler_kwargs = {\"num_cycles\": 1},\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "    # Create Trainer instance\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"validation\"],\n",
        "        compute_metrics=compute_metrics,\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    trainer.model.save_pretrained(repository_id)\n",
        "    merged_model = model.merge_and_unload()\n",
        "\n",
        "    return merged_model, tokenizer, trainer\n",
        "def read_json(path):\n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(data, path):\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "def get_prediction(model,tokenizer, prompt, length=250,stype='greedy'):\n",
        "\n",
        "    inputs = tokenizer(prompt, add_special_tokens=True, max_length=4096,return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(inputs, max_new_tokens=length)\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wHFZlufnBlGQ"
      },
      "outputs": [],
      "source": [
        "def read_json(path):\n",
        "    \"\"\" Read a json file from the given path.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def write_json(data, path):\n",
        "    \"\"\" Write a json file to the given path.\"\"\"\n",
        "    if not os.path.exists(os.path.dirname(path)):\n",
        "        os.makedirs(os.path.dirname(path))\n",
        "\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5QETADxqEvEk"
      },
      "outputs": [],
      "source": [
        "##kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "#send data emmbeddings according to relation types.\n",
        "def sample_selection_kmeans(data, memory_size):\n",
        "    # Extract the embeddings and convert them into a 2D numpy array\n",
        "    embeddings_array = [item['embedding'] for item in data]\n",
        "    prompt_array = [item['prompt'] for item in data]\n",
        "    relation_array = [item['relation'] for item in data]\n",
        "\n",
        "    min_dim = min(emb.shape[1] for emb in embeddings_array)\n",
        "\n",
        "    reshaped_embeddings = [emb[:, :min_dim].reshape(-1) for emb in embeddings_array]\n",
        "    embeddings_array = np.vstack(reshaped_embeddings)\n",
        "\n",
        "    num_clusters = min(memory_size, len(embeddings_array))\n",
        "\n",
        "    distances = KMeans(n_clusters=num_clusters, random_state=0).fit_transform(embeddings_array)\n",
        "\n",
        "    mem_set = [] # Initialize mem_set as a list\n",
        "    proto_set = []\n",
        "    relations = []\n",
        "    selected_samples = []\n",
        "\n",
        "    for i in range(num_clusters):\n",
        "        sel_index = np.argmin(distances[:,i])\n",
        "        instance = embeddings_array[sel_index]\n",
        "        mem_set.append(instance)\n",
        "        proto = prompt_array[sel_index]\n",
        "        proto_set.append(proto)\n",
        "        relations.append(relation_array[sel_index])\n",
        "        print(relation_array[sel_index])\n",
        "        selected_samples.append({\"prompt\":proto,\"relation\":relation_array[sel_index]})\n",
        "\n",
        "\n",
        "    mem_set = np.array(mem_set)\n",
        "\n",
        "    return mem_set, proto_set, relations, selected_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IJAmobReEyYS"
      },
      "outputs": [],
      "source": [
        "def compute_embedding(model, tokenizer, input_path):\n",
        "  data = read_json(input_path)\n",
        "  embeddings = []\n",
        "  for i, item in enumerate(data):\n",
        "    prompt = item['prompt']\n",
        "    relation  = item['relation']\n",
        "    inputs = tokenizer(prompt, add_special_tokens=True, max_length=4096,return_tensors=\"pt\").input_ids.to(\"cuda\") # Move inputs to GPU\n",
        "    embedding = model.encoder(inputs)\n",
        "    embeddings.append({\"prompt\": prompt, \"relation\":relation, \"embedding\": embedding['last_hidden_state'].data.cpu().numpy()}) # Move embedding back to CPU for NumPy\n",
        "  return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t-3ig6g3QcmN"
      },
      "outputs": [],
      "source": [
        "def select_samples(model, tokenizer, memory_size, train_data_path, tasks_path):\n",
        "  ### TODO ###\n",
        "  relations = read_json(tasks_path)\n",
        "\n",
        "  embeddings = compute_embedding(model, tokenizer, train_data_path)\n",
        "  r1_emb = [ emb for emb in embeddings if emb['relation'] == relations[-1]]\n",
        "  r2_emb = [ emb for emb in embeddings if emb['relation'] == relations[-2]]\n",
        "  r3_emb = [ emb for emb in embeddings if emb['relation'] == relations[-3]]\n",
        "  r4_emb = [ emb for emb in embeddings if emb['relation'] == relations[-4]]\n",
        "  r5_emb = [ emb for emb in embeddings if emb['relation'] == relations[-5]]\n",
        "  if len(relations) == 6:\n",
        "    r6_emb = [ emb for emb in embeddings if emb['relation'] == relations[-6]]\n",
        "  mem1, proto_set1, relations1,selected_samples1 = sample_selection_kmeans(r1_emb, memory_size)\n",
        "  mem2, proto_set2, relations2, selected_samples2 = sample_selection_kmeans(r2_emb,memory_size)\n",
        "  mem3, proto_set3, relations3, selected_samples3 = sample_selection_kmeans(r3_emb, memory_size)\n",
        "  mem4, proto_set4, relations4, selected_samples4 = sample_selection_kmeans(r4_emb, memory_size)\n",
        "  mem5, proto_set5, relations5, selected_samples5 = sample_selection_kmeans(r5_emb, memory_size)\n",
        "  if len(relations) == 6:\n",
        "    mem6, proto_set6, relations6, selected_samples6 = sample_selection_kmeans(r6_emb, memory_size)\n",
        "  all_selected_samples = []\n",
        "  all_selected_samples.extend(selected_samples1)\n",
        "  all_selected_samples.extend(selected_samples2)\n",
        "  all_selected_samples.extend(selected_samples3)\n",
        "  all_selected_samples.extend(selected_samples4)\n",
        "  all_selected_samples.extend(selected_samples5)\n",
        "  if len(relations) == 6:\n",
        "    all_selected_samples.extend(selected_samples6)\n",
        "\n",
        "\n",
        "\n",
        "  return all_selected_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "L47j0Z3shXLm"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(experiment_id, task_id, model, tokenizer, current_task=True):\n",
        "    if current_task:\n",
        "      input_path = \"tacred/5way5shot-test/run{0}/{1}/test.json\".format(experiment_id, task_id)\n",
        "      data = read_json(input_path)\n",
        "      out_pred_path = \"KMmeans_CRE_tacred{0}/task_{1}_current_task_pred.json\".format(experiment_id, task_id)\n",
        "      out_acc_path = \"KMmeans_CRE_tacred{0}/task_{1}_current_task_result.json\".format(experiment_id, task_id)\n",
        "    else:\n",
        "\n",
        "      data = []\n",
        "      for i in range(1, task_id+1):\n",
        "        input_path = \"tacred/5way5shot-test/run{0}/task{1}/test.json\".format(experiment_id, i)\n",
        "        data.extend(read_json(input_path))\n",
        "      out_pred_path = \"KMmeans_CRE_tacred_{0}/task_{1}_seen_task.json\".format(experiment_id, task_id)\n",
        "      out_acc_path = \"KMmeans_CRE_tacred_{0}/task_{1}_seen_task_result.json\".format(experiment_id, task_id)\n",
        "    responses = []\n",
        "    relations = []\n",
        "    for j, item in enumerate(data):\n",
        "      prompt = item['prompt']\n",
        "      relations.append(item['relation'])\n",
        "\n",
        "      response = get_prediction(model, tokenizer, prompt)\n",
        "      print('test:', j)\n",
        "      if len(response) == 0:\n",
        "          print(\"No response\")\n",
        "          responses.append(\"\")\n",
        "      else:\n",
        "          print(response[0])\n",
        "          responses.append({\"predict\":response[0]})\n",
        "    y_true = relations\n",
        "    preds = [line['predict'] for line in responses]\n",
        "    acc = accuracy_score(y_true, preds)\n",
        "    result = [{\"acc\":acc}]\n",
        "\n",
        "    write_json(responses, out_pred_path)\n",
        "    write_json(result,out_acc_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6i01s4KQx-TH"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "U9SpudOIw8jy"
      },
      "outputs": [],
      "source": [
        "logs = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SCkAsF_2Ux_5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "m=1\n",
        "for experiment_id in range(1,2):\n",
        "\n",
        "  print(\"Experiment: {0}\".format(experiment_id))\n",
        "\n",
        "  dataset_path = 'tacred/5way5shot/run{0}/task1/'.format(experiment_id)\n",
        "  tasks_path = \"relations/run_{0}/task1.json\".format(experiment_id)\n",
        "  model_id=\"google/flan-t5-base\"\n",
        "  task_id = \"task1\"\n",
        "  targets_labels = []\n",
        "  max_source_length = None\n",
        "  max_target_length = None\n",
        "  tokenizer = set_tokenizer()\n",
        "  print(dataset_path)\n",
        "  lora_config = LoraConfig(\n",
        "                  # the task to train for (sequence-to-sequence language modeling in this case)\n",
        "                  task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "                  # the dimension of the low-rank matrices\n",
        "                  r=4,\n",
        "                  # the scaling factor for the low-rank matrices\n",
        "                  lora_alpha=32,\n",
        "                  # the dropout probability of the LoRA layers\n",
        "                  lora_dropout=0.01,\n",
        "                  target_modules=[\"k\",\"q\",\"v\",\"o\"]\n",
        "                  )\n",
        "\n",
        "  metric = evaluate.load(\"rouge\")\n",
        "  start_time = datetime.now()\n",
        "  model, tokenizer, trainer = main(model_id, dataset_path, tasks_path, task_id, False)\n",
        "  end_time = datetime.now()\n",
        "  train_time = 'Base Train. Experiment Id: {0}. Task Id: {1}. Duration: {2} \\n'.format(experiment_id, task_id, end_time - start_time)\n",
        "  logs += train_time\n",
        "\n",
        "  model.save_pretrained(\"KMmeans_CRE_tacred_{0}/FSCRE_5_1/\".format(experiment_id), from_pt=True)\n",
        "  ## evaluate model\n",
        "  evaluate_model(experiment_id, task_id, model, tokenizer, current_task=True)\n",
        "  if m >0:\n",
        "      train_data_path = dataset_path+\"train_1.json\"\n",
        "      all_selected_samples = select_samples(model, tokenizer, m, train_data_path, tasks_path)\n",
        "\n",
        "      for k in range(2, 11):\n",
        "        outpath_selected_samples = 'tacred/5way5shot/memory/run_{0}/task{1}/train_2.json'.format(experiment_id,k)\n",
        "        write_json(all_selected_samples, outpath_selected_samples)\n",
        "\n",
        "  for i in range(1, 10):\n",
        "    targets_labels = []\n",
        "    max_source_length = None\n",
        "    max_target_length = None\n",
        "    tokenizer = set_tokenizer()\n",
        "    metric = evaluate.load(\"rouge\")\n",
        "\n",
        "    dataset_path = 'tacred/5way5shot/run_{0}/task{1}/'.format(experiment_id,i+1)\n",
        "    tasks_path = \"relations/run{0}/task{1}.json\".format(experiment_id,i+1)\n",
        "\n",
        "    base_model_id=\"KMmeans_CRE_tacred_{0}/FSCRE_5_{1}/\".format(experiment_id, i)\n",
        "    task_id = \"task{0}\".format(i+1)\n",
        "\n",
        "    print(base_model_id)\n",
        "    start_time = datetime.now()\n",
        "    model, tokenizer, trainer = main(base_model_id, dataset_path, tasks_path, task_id, True)\n",
        "    end_time = datetime.now()\n",
        "    train_time = 'Base Train. Experiment Id: {0}. Task Id: {1}. Duration: {2} \\n'.format(experiment_id, task_id, end_time - start_time)\n",
        "    logs += train_time\n",
        "    model.save_pretrained(\"KMmeans_CRE_tacred_{0}/FSCRE_5_{1}/\".format(experiment_id, i+1), from_pt=True)\n",
        "    write_json(logs, \"KMmeans_CRE_tacred_{0}/logs.txt\".format(experiment_id))\n",
        "    if m > 0:\n",
        "        if i <9:\n",
        "          train_data_path = dataset_path+\"train_1.json\"\n",
        "          all_selected_samples = select_samples(model, tokenizer,m, train_data_path, tasks_path)\n",
        "          for j in range(i+2, 11):\n",
        "            outpath_selected_samples = 'tacred/5way5shot/memory/run_{0}/task{1}/train_{2}.json'.format(experiment_id, j, i+2)\n",
        "            write_json(all_selected_samples, outpath_selected_samples)\n",
        "\n",
        "        ########################### Memory Train ######################################\n",
        "        targets_labels = []\n",
        "        max_source_length = None\n",
        "        max_target_length = None\n",
        "        tokenizer = set_tokenizer()\n",
        "        metric = evaluate.load(\"rouge\")\n",
        "\n",
        "        dataset_path = 'tacred/5way5shot/memory/run_{0}/task{1}/'.format(experiment_id,i+1)\n",
        "        tasks_path = \"relations/run_{0}/task{1}.json\".format(experiment_id,i+1)\n",
        "\n",
        "        base_model_id = \"KMmeans_CRE_tacred_{0}/FSCRE_5_{1}/\".format(experiment_id, i+1)\n",
        "        task_id = \"task{0}\".format(i+1)\n",
        "\n",
        "        print(base_model_id)\n",
        "        start_time = datetime.now()\n",
        "        model, tokenizer, trainer = main(base_model_id, dataset_path, tasks_path, task_id, True)\n",
        "        end_time = datetime.now()\n",
        "\n",
        "        train_time = 'Memory Train. Experiment Id: {0}. Task Id: {1}. Duration: {2} \\n'.format(experiment_id, task_id, end_time - start_time)\n",
        "        logs += train_time\n",
        "        model.save_pretrained(\"KMmeans_CRE_tacred_{0}/FSCRE_5_{1}/\".format(experiment_id, i+1), from_pt=True)\n",
        "        ### evaluate model\n",
        "        evaluate_model(experiment_id, i+1, model, tokenizer, current_task=False)\n",
        "        write_json(logs, \"KMmeans_CRE_tacred_{0}/logs.txt\".format(experiment_id))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx3bAqLst9CN",
        "outputId": "3db9f442-0394-4673-9e1b-115933c6442e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_dataset(\"tacred/5way5shot/train/run1/task1/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "55cDYxmtOLv2",
        "outputId": "10b76011-3b01-4912-c2d6-58686fd2a2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find any data file at /content/tacred/5way5shot/train/run1/task1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a2754768baab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tacred/5way5shot/train/run1/task1/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-72ee5ba1d9b7>\u001b[0m in \u001b[0;36mset_dataset\u001b[0;34m(dataset_id)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2130\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         )\n\u001b[1;32m   1736\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1737\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any data file at {relative_to_absolute_path(path)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any data file at /content/tacred/5way5shot/train/run1/task1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beAdIPwHlJeK"
      },
      "outputs": [],
      "source": [
        "write_json(logs, \"KMmeans_CRE_tacred_{0}/logs.txt\".format(experiment_id))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}